# Databricks notebook source
# MAGIC %pip install --quiet transformers==4.30.2 "unstructured[pdf,docx]==0.10.30" llama-index==0.9.3 pydantic==1.10.9 mlflow==2.12.1
# MAGIC
# MAGIC dbutils.library.restartPython()

# COMMAND ----------

import time
from unstructured.partition.auto import partition
import re
import io
import nltk

from mlflow.deployments import get_deploy_client
from pprint import pprint

nltk.download('punkt_tab')
nltk.download('averaged_perceptron_tagger_eng')

# COMMAND ----------

# Reduce the arrow batch size as our PDF can be big in memory
spark.conf.set("spark.sql.execution.arrow.maxRecordsPerBatch", 10)
articles_path_1 = f"dbfs:/mnt/uc/src/landing/rag_content/pdf/jewelry_guide"

df = (spark.read.format("binaryfile")
                .option("recursiveFileLookup", "true")
                .load(articles_path_1))

# COMMAND ----------

# save list of the files to table
table_name = "temp_catalog.temp_schema.pdf_raw_text"
if not spark.catalog.tableExists(table_name):
    (df.write.mode("overwrite")
            .option("overwriteSchema", "true")
            .saveAsTable(table_name))
else:
    df.write.mode("append").saveAsTable(table_name)
    
# COMMAND ----------

def extract_doc_text(x : bytes) -> str:
  # Read files and extract the values with unstructured
  sections = partition(file=io.BytesIO(x))
  def clean_section(txt):
    txt = re.sub(r'\n', '', txt)
    return re.sub(r' ?\.', '.', txt)
  # Default split is by section of document
  # concatenate them all together because we want to split by sentence instead.
  return "\n".join([clean_section(s.text) for s in sections])

# COMMAND ----------

import io
import os
import pandas as pd 

from llama_index.langchain_helpers.text_splitter import SentenceSplitter
from llama_index import Document, set_global_tokenizer
from transformers import AutoTokenizer
from typing import Iterator
from pyspark.sql.functions import col, length, explode
from unstructured.partition.auto import partition

# Function to extract and split text
def extract_and_split(content):
    # Set llama2 as tokenizer
    set_global_tokenizer(
        AutoTokenizer.from_pretrained("hf-internal-testing/llama-tokenizer")
    )

    # Sentence splitter from llama_index to split on sentences
    splitter = SentenceSplitter(chunk_size=500, chunk_overlap=50)
    
    txt = extract_doc_text(content)
    nodes = splitter.get_nodes_from_documents([Document(text=txt)])
    return [n.text for n in nodes]

df_pandas = df.toPandas()
df_pandas['content'] = df_pandas['content'].apply(extract_and_split)
df_pandas = df_pandas.explode('content')

df_pandas = df_pandas[['path', 'content']]
df_pandas.rename(columns={'path': 'pdf_name'}, inplace=True)
df_chunks = spark.createDataFrame(df_pandas)
display(df_chunks)

# COMMAND ----------

deploy_client = get_deploy_client("databricks")

# COMMAND ----------

from pyspark.sql.functions import pandas_udf
@pandas_udf("array<float>")
def get_embedding(contents: pd.Series) -> pd.Series:
    import mlflow.deployments
    deploy_client = mlflow.deployments.get_deploy_client("databricks")
    def get_embeddings(batch):
        #Note: this will fail if an exception is thrown during embedding creation (add try/except if needed) 
        response = deploy_client.predict(endpoint="databricks-gte-large-en", inputs={"input": batch})
        return [e['embedding'] for e in response.data]

    # Splitting the contents into batches of 150 items each, since the embedding model takes at most 150 inputs per request.
    max_batch_size = 150
    batches = [contents.iloc[i:i + max_batch_size] for i in range(0, len(contents), max_batch_size)]

    # Process each batch and collect the results
    all_embeddings = []
    for batch in batches:
        all_embeddings += get_embeddings(batch.tolist())

    return pd.Series(all_embeddings)

# COMMAND ----------

import pyspark.sql.functions as F

df_chunk_emd = (df_chunks
                .withColumn("embedding", get_embedding("content"))
                .selectExpr('pdf_name', 'content', 'embedding')
                )
display(df_chunk_emd)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Save Embeddings to a Delta Table
# MAGIC
# MAGIC Now that the embeddings are ready, let's create a Delta table and store the embeddings in this table.

# COMMAND ----------

# MAGIC %sql
# MAGIC CREATE TABLE IF NOT EXISTS temp_catalog.temp_schema.pdf_text_embeddings (
# MAGIC   id BIGINT GENERATED BY DEFAULT AS IDENTITY,
# MAGIC   pdf_name STRING,
# MAGIC   content STRING,
# MAGIC   embedding ARRAY <FLOAT>
# MAGIC   ) TBLPROPERTIES (delta.enableChangeDataFeed = true);
# MAGIC

# COMMAND ----------

df_chunk_emd.write.mode("append").saveAsTable("temp_catalog.temp_schema.pdf_text_embeddings")

# COMMAND ----------

